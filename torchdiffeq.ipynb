{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torchdiffeq.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "14qmTldM9RMbCFXmzpsg4zWqAjUVYAno8",
      "authorship_tag": "ABX9TyMWeoaFzRnELF5EsdO0BNO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tulnertje5/engineer/blob/master/torchdiffeq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQcffkBI-d9O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvSeoBngBe7E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU8r67VpEWcx"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive/\")\r\n",
        "\r\n",
        "%cd /content/drive/MyDrive/Afstudeer_project/torchdiffeq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKgPE75AG5bA"
      },
      "source": [
        "!pip install torchdiffeq\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "import logging\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn.functional as F\r\n",
        "import matplotlib.pyplot as plt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18yDYtngIhTQ"
      },
      "source": [
        "class Linear_Experiment(nn.Module):\r\n",
        "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = x(A^T-A) + b`\r\n",
        "    Args:\r\n",
        "        in_features: size of each input sample\r\n",
        "        out_features: size of each output sample\r\n",
        "        bias: If set to ``False``, the layer will not learn an additive bias.\r\n",
        "            Default: ``True``\r\n",
        "    Shape:\r\n",
        "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\r\n",
        "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\r\n",
        "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\r\n",
        "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\r\n",
        "    Attributes:\r\n",
        "        weight: the learnable weights of the module of shape\r\n",
        "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\r\n",
        "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\r\n",
        "            :math:`k = \\frac{1}{\\text{in\\_features}}`\r\n",
        "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\r\n",
        "                If :attr:`bias` is ``True``, the values are initialized from\r\n",
        "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\r\n",
        "                :math:`k = \\frac{1}{\\text{in\\_features}}`\r\n",
        "    Examples::\r\n",
        "        >>> m = Linear_Experiment(20, 30)\r\n",
        "        >>> input = torch.randn(128, 20)\r\n",
        "        >>> output = m(input)\r\n",
        "        >>> print(output.size())\r\n",
        "        torch.Size([128, 30])\r\n",
        "    \"\"\"\r\n",
        "    # __constants__ = ['in_features', 'out_features']\r\n",
        "    # in_features: int\r\n",
        "    # out_features: int\r\n",
        "    # weight: Tensor\r\n",
        "\r\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\r\n",
        "        super(Linear_Experiment, self).__init__()\r\n",
        "        self.in_features = in_features\r\n",
        "        self.out_features = out_features\r\n",
        "        self.weight = torch.rand(out_features, in_features ,  requires_grad=True)\r\n",
        "        if bias:\r\n",
        "            self.bias = torch.rand(out_features , requires_grad=True)\r\n",
        "        else:\r\n",
        "            self.register_parameter('bias', None)\r\n",
        "        # self.reset_parameters()\r\n",
        "\r\n",
        "    # def reset_parameters(self) -> None:\r\n",
        "    #     init.kaiming_uniform_(self.weight, a=math.sqrt(5))\r\n",
        "    #     if self.bias is not None:\r\n",
        "    #         fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\r\n",
        "    #         bound = 1 / math.sqrt(fan_in)\r\n",
        "    #         init.uniform_(self.bias, -bound, bound)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return F.linear(input, self.weight-torch.transpose(self.weight,0,1), self.bias)\r\n",
        "\r\n",
        "    def extra_repr(self) -> str:\r\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\r\n",
        "            self.in_features, self.out_features, self.bias is not None\r\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzWihxZloZTj"
      },
      "source": [
        "class Args:\r\n",
        "  def __init__(self,network='odenet', tol=1e-3,adjoint=True,downsampling_method='conv',nepochs=5,data_aug=True,lr=0.1,batch_size=254,test_batch_size=1000,save='./experimet1',gpu=0 ):\r\n",
        "    self.network=network\r\n",
        "    self.tol=tol\r\n",
        "    self.adjoint=adjoint\r\n",
        "    self.downsampling_method=downsampling_method\r\n",
        "    self.nepochs=nepochs\r\n",
        "    self.data_aug=data_aug\r\n",
        "    self.lr=lr\r\n",
        "    self.batch_size=batch_size\r\n",
        "    self.test_batch_size=test_batch_size\r\n",
        "    self.save=save\r\n",
        "    self.gpu=gpu\r\n",
        "args=Args()\r\n",
        "\r\n",
        "\r\n",
        "if args.adjoint:\r\n",
        "    from torchdiffeq import odeint_adjoint as odeint\r\n",
        "else:\r\n",
        "    from torchdiffeq import odeint\r\n",
        "\r\n",
        "def conv3x3(in_planes, out_planes, stride=1):\r\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\r\n",
        "\r\n",
        "\r\n",
        "def conv1x1(in_planes, out_planes, stride=1):\r\n",
        "    \"\"\"1x1 convolution\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\r\n",
        "\r\n",
        "\r\n",
        "def norm(dim):\r\n",
        "    return nn.GroupNorm(min(32, dim), dim)\r\n",
        "\r\n",
        "class ResBlock(nn.Module):\r\n",
        "    expansion = 1\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n",
        "        super(ResBlock, self).__init__()\r\n",
        "        self.norm1 = norm(inplanes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
        "        self.norm2 = norm(planes)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        shortcut = x\r\n",
        "\r\n",
        "        out = self.relu(self.norm1(x))\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            shortcut = self.downsample(out)\r\n",
        "\r\n",
        "        out = self.conv1(out)\r\n",
        "        out = self.norm2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.conv2(out)\r\n",
        "\r\n",
        "        return out + shortcut\r\n",
        "\r\n",
        "\r\n",
        "class ConcatConv2d(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\r\n",
        "        super(ConcatConv2d, self).__init__()\r\n",
        "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\r\n",
        "        self._layer = module(\r\n",
        "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\r\n",
        "            bias=bias\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, t, x):\r\n",
        "        tt = torch.ones_like(x[:, :1, :, :]) * t\r\n",
        "        ttx = torch.cat([tt, x], 1)\r\n",
        "        return self._layer(ttx)\r\n",
        "\r\n",
        "\r\n",
        "class ODEfunc(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, dim):\r\n",
        "        super(ODEfunc, self).__init__()\r\n",
        "        self.norm1 = norm(dim)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\r\n",
        "        self.norm2 = norm(dim)\r\n",
        "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\r\n",
        "        self.norm3 = norm(dim)\r\n",
        "        self.nfe = 0\r\n",
        "\r\n",
        "    def forward(self, t, x):\r\n",
        "        self.nfe += 1\r\n",
        "        out = self.norm1(x)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.conv1(t, out)\r\n",
        "        out = self.norm2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.conv2(t, out)\r\n",
        "        out = self.norm3(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "class ODEBlock(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, odefunc):\r\n",
        "        super(ODEBlock, self).__init__()\r\n",
        "        self.odefunc = odefunc\r\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        self.integration_time = self.integration_time.type_as(x)\r\n",
        "        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\r\n",
        "        return out[1]\r\n",
        "\r\n",
        "    @property\r\n",
        "    def nfe(self):\r\n",
        "        return self.odefunc.nfe\r\n",
        "\r\n",
        "    @nfe.setter\r\n",
        "    def nfe(self, value):\r\n",
        "        self.odefunc.nfe = value\r\n",
        "\r\n",
        "\r\n",
        "class Flatten(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(Flatten, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\r\n",
        "        return x.view(-1, shape)\r\n",
        "\r\n",
        "class RunningAverageMeter(object):\r\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, momentum=0.99):\r\n",
        "        self.momentum = momentum\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.val = None\r\n",
        "        self.avg = 0\r\n",
        "\r\n",
        "    def update(self, val):\r\n",
        "        if self.val is None:\r\n",
        "            self.avg = val\r\n",
        "        else:\r\n",
        "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\r\n",
        "        self.val = val\r\n",
        "\r\n",
        "\r\n",
        "def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\r\n",
        "    if data_aug:\r\n",
        "        transform_train = transforms.Compose([\r\n",
        "            transforms.RandomCrop(28, padding=4),\r\n",
        "            transforms.ToTensor(),\r\n",
        "        ])\r\n",
        "    else:\r\n",
        "        transform_train = transforms.Compose([\r\n",
        "            transforms.ToTensor(),\r\n",
        "        ])\r\n",
        "\r\n",
        "    transform_test = transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "    ])\r\n",
        "\r\n",
        "    train_loader = DataLoader(\r\n",
        "        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\r\n",
        "        shuffle=True, num_workers=2, drop_last=True\r\n",
        "    )\r\n",
        "\r\n",
        "    train_eval_loader = DataLoader(\r\n",
        "        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\r\n",
        "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\r\n",
        "    )\r\n",
        "\r\n",
        "    test_loader = DataLoader(\r\n",
        "        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\r\n",
        "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\r\n",
        "    )\r\n",
        "\r\n",
        "    return train_loader, test_loader, train_eval_loader\r\n",
        "\r\n",
        "\r\n",
        "def inf_generator(iterable):\r\n",
        "    \"\"\"Allows training with DataLoaders in a single infinite loop:\r\n",
        "        for i, (x, y) in enumerate(inf_generator(train_loader)):\r\n",
        "    \"\"\"\r\n",
        "    iterator = iterable.__iter__()\r\n",
        "    while True:\r\n",
        "        try:\r\n",
        "            yield iterator.__next__()\r\n",
        "        except StopIteration:\r\n",
        "            iterator = iterable.__iter__()\r\n",
        "\r\n",
        "\r\n",
        "def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\r\n",
        "    initial_learning_rate = args.lr * batch_size / batch_denom\r\n",
        "\r\n",
        "    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\r\n",
        "    vals = [initial_learning_rate * decay for decay in decay_rates]\r\n",
        "\r\n",
        "    def learning_rate_fn(itr):\r\n",
        "        lt = [itr < b for b in boundaries] + [True]\r\n",
        "        i = np.argmax(lt)\r\n",
        "        return vals[i]\r\n",
        "\r\n",
        "    return learning_rate_fn\r\n",
        "\r\n",
        "\r\n",
        "def one_hot(x, K):\r\n",
        "    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\r\n",
        "\r\n",
        "\r\n",
        "def accuracy(model, dataset_loader):\r\n",
        "    total_correct = 0\r\n",
        "    with torch.no_grad():\r\n",
        "      for x, y in dataset_loader:\r\n",
        "          x = x.to(device)\r\n",
        "          y = one_hot(np.array(y.numpy()), 10)\r\n",
        "          target_class = np.argmax(y, axis=1)\r\n",
        "          predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\r\n",
        "          total_correct += np.sum(predicted_class == target_class)\r\n",
        "    return total_correct / len(dataset_loader.dataset)\r\n",
        "\r\n",
        "\r\n",
        "def count_parameters(model):#neet to optimize\r\n",
        "  param=0\r\n",
        "  for _ ,p in model.named_parameters():\r\n",
        "    if p.requires_grad:\r\n",
        "      param+=torch.sum(p!=0).item()\r\n",
        "  return param\r\n",
        "\r\n",
        "\r\n",
        "def makedirs(dirname):\r\n",
        "    if not os.path.exists(dirname):\r\n",
        "        os.makedirs(dirname)\r\n",
        "\r\n",
        "\r\n",
        "def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\r\n",
        "    logger = logging.getLogger()\r\n",
        "    if debug:\r\n",
        "        level = logging.DEBUG\r\n",
        "    else:\r\n",
        "        level = logging.INFO\r\n",
        "    logger.setLevel(level)\r\n",
        "    if saving:\r\n",
        "        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\r\n",
        "        info_file_handler.setLevel(level)\r\n",
        "        logger.addHandler(info_file_handler)\r\n",
        "    if displaying:\r\n",
        "        console_handler = logging.StreamHandler()\r\n",
        "        console_handler.setLevel(level)\r\n",
        "        logger.addHandler(console_handler)\r\n",
        "    logger.info(filepath)\r\n",
        "    with open(filepath, \"r\") as f:\r\n",
        "        logger.info(f.read())\r\n",
        "\r\n",
        "    for f in package_files:\r\n",
        "        logger.info(f)\r\n",
        "        with open(f, \"r\") as package_f:\r\n",
        "            logger.info(package_f.read())\r\n",
        "\r\n",
        "    return logger\r\n",
        "\r\n",
        "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "is_odenet = args.network == 'odenet'\r\n",
        "\r\n",
        "class Ourmodel(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "      super(Ourmodel,self).__init__()\r\n",
        "      if args.downsampling_method == 'conv':\r\n",
        "        downsampling_layers = [\r\n",
        "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1 , padding=0),     #so we get a 64*26*26 tensor\r\n",
        "            norm(64),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),     # gives back a 64*12*12 tensor\r\n",
        "            norm(64),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),     #gives a 62*5*5 tensor\r\n",
        "            ]\r\n",
        "      elif args.downsampling_method == 'res':\r\n",
        "          downsampling_layers = [\r\n",
        "              nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1),             \r\n",
        "              ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\r\n",
        "              ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\r\n",
        "          ]\r\n",
        "\r\n",
        "      feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\r\n",
        "      fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\r\n",
        "\r\n",
        "      self.model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    out=self.model(x)\r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuNX4ZHGTNM5"
      },
      "source": [
        "input = torch.empty(2, 35)\r\n",
        "tt=torch.ones_like(input[:,:1])\r\n",
        "ttx=torch.cat([tt, input], 1)\r\n",
        "print(ttx.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7hwuWzkamkJ"
      },
      "source": [
        "# Model creation with htan inear\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOF7rqRDG6XN"
      },
      "source": [
        "\r\n",
        "class ODEfunc_experiment(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,dim=64):\r\n",
        "        super(ODEfunc_experiment, self).__init__()\r\n",
        "        self.Linear=nn.Linear(dim+2, dim)\r\n",
        "        self.Tanh=nn.Tanh()\r\n",
        "        self.nfe = 0\r\n",
        "\r\n",
        "    def forward(self, t, x):\r\n",
        "        self.nfe += 1\r\n",
        "        tt = torch.ones_like(x[:, :1]) * t\r\n",
        "        tt2 = torch.ones_like(x[:, :1]) * t * t\r\n",
        "        ttx = torch.cat([tt,tt2,  x], 1)\r\n",
        "        out=self.Linear(ttx)\r\n",
        "        out=self.Tanh(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class ODEfunc_experiment_transpose(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,dim=64):\r\n",
        "        super(ODEfunc_experiment_transpose, self).__init__()\r\n",
        "        self.Linear=Linear_Experiment(dim+2, dim)\r\n",
        "        self.Tanh=nn.Tanh()\r\n",
        "        self.nfe = 0\r\n",
        "\r\n",
        "    def forward(self, t, x):\r\n",
        "        self.nfe += 1\r\n",
        "        tt = torch.ones_like(x[:, :1]) * t\r\n",
        "        tt2 = torch.ones_like(x[:, :1]) * t * t\r\n",
        "        ttx = torch.cat([tt,tt2,  x], 1)\r\n",
        "        out=self.Linear(ttx)\r\n",
        "        out=self.Tanh(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "ODEfunc_experiment = ODEfunc_experiment()\r\n",
        "ODEfunc_experiment_transpose=ODEfunc_experiment_transpose()\r\n",
        "def downsampling_layers():\r\n",
        "    if args.downsampling_method == 'conv':\r\n",
        "            downsampling_layers = [\r\n",
        "                nn.Conv2d(in_channels=1, out_channels=32, kernel_size=6, stride=2 , padding=0),       #so we get a 32*11*11 tensor\r\n",
        "                norm(32),\r\n",
        "                nn.ReLU(inplace=True),\r\n",
        "                nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=0),       # gives back a 64*5*5 tensor\r\n",
        "                norm(32),\r\n",
        "                nn.ReLU(inplace=True),\r\n",
        "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0),       # gives back a 128*1*1 tensor\r\n",
        "                Flatten()\r\n",
        "                ]\r\n",
        "    elif args.downsampling_method == 'res':\r\n",
        "            downsampling_layers = [\r\n",
        "                nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1),             \r\n",
        "                ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\r\n",
        "                ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\r\n",
        "            ]\r\n",
        "    return downsampling_layers\r\n",
        "\r\n",
        "feature_layers_normal = [ODEBlock(ODEfunc_experiment)]\r\n",
        "feature_layers_transpose = [ODEBlock(ODEfunc_experiment_transpose)]\r\n",
        "\r\n",
        "fc_layers_1 = [nn.Linear(64, 10)]\r\n",
        "fc_layers_2 = [nn.Linear(64, 10)]\r\n",
        "fc_layers_3 = [nn.Linear(64, 10)]\r\n",
        "\r\n",
        "downsampling_layers_1=downsampling_layers()\r\n",
        "downsampling_layers_2=downsampling_layers()\r\n",
        "downsampling_layers_3=downsampling_layers()\r\n",
        "\r\n",
        "model_baseline  = nn.Sequential(*downsampling_layers_1, *fc_layers_1).to(device)\r\n",
        "model_normal = nn.Sequential(*downsampling_layers_2, *feature_layers_normal, *fc_layers_2).to(device)\r\n",
        "model_transpose = nn.Sequential(*downsampling_layers_3, *feature_layers_transpose, *fc_layers_3).to(device)\r\n",
        "model_transpose[8].odefunc.Linear.weight=model_transpose[8].odefunc.Linear.weight.to(device)\r\n",
        "model_transpose[8].odefunc.Linear.bias=model_transpose[8].odefunc.Linear.bias.to(device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zaZlBATAoIb"
      },
      "source": [
        "args=Args(network='odenet', tol=1e-3,adjoint=True,downsampling_method='conv',nepochs=75,data_aug=True,lr=0.0005,batch_size=128,test_batch_size=500,save='./experimet1',gpu=0 )\r\n",
        "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\r\n",
        "is_odenet = args.network == 'odenet'\r\n",
        "\r\n",
        "def fit_model(model):\r\n",
        "  criterion = nn.CrossEntropyLoss().to(device)\r\n",
        "\r\n",
        "  train_loader, test_loader, train_eval_loader = get_mnist_loaders(\r\n",
        "      args.data_aug, args.batch_size, args.test_batch_size\r\n",
        "  )\r\n",
        "\r\n",
        "  data_gen = inf_generator(train_loader)\r\n",
        "  batches_per_epoch = len(train_loader)\r\n",
        "\r\n",
        "  lr_fn = learning_rate_with_decay(\r\n",
        "      args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\r\n",
        "      decay_rates=[1, 0.1, 0.01, 0.01]\r\n",
        "  )\r\n",
        "\r\n",
        "  # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=1)\r\n",
        "  optimizer=torch.optim.Adagrad(model.parameters() ,lr=args.lr)\r\n",
        "  best_acc = 0\r\n",
        "  batch_time_meter = RunningAverageMeter()\r\n",
        "  f_nfe_meter = RunningAverageMeter()\r\n",
        "  b_nfe_meter = RunningAverageMeter()\r\n",
        "  end = time.time()\r\n",
        "  test_acc_array=[]\r\n",
        "  avrage_loss_array=[]\r\n",
        "  avrage_loss=0\r\n",
        "  for itr in range(args.nepochs * batches_per_epoch):\r\n",
        "\r\n",
        "          for param_group in optimizer.param_groups:\r\n",
        "              param_group['lr'] = lr_fn(itr)\r\n",
        "\r\n",
        "          optimizer.zero_grad()\r\n",
        "          x, y = data_gen.__next__()\r\n",
        "          x = x.to(device)\r\n",
        "          y = y.to(device)\r\n",
        "          logits = model(x)\r\n",
        "          print(\"hoi\")\r\n",
        "          loss = criterion(logits, y)\r\n",
        "\r\n",
        "          # if is_odenet:\r\n",
        "          #     nfe_forward = feature_layers[0].nfe\r\n",
        "          #     feature_layers[0].nfe = 0\r\n",
        "\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "          avrage_loss+=loss\r\n",
        "\r\n",
        "          # if is_odenet:\r\n",
        "          #     nfe_backward = feature_layers[0].nfe\r\n",
        "          #     feature_layers[0].nfe = 0\r\n",
        "\r\n",
        "          batch_time_meter.update(time.time() - end)\r\n",
        "          # if is_odenet:\r\n",
        "          #     f_nfe_meter.update(nfe_forward)\r\n",
        "          #     b_nfe_meter.update(nfe_backward)\r\n",
        "          end = time.time()\r\n",
        "\r\n",
        "          if itr % batches_per_epoch == 0:\r\n",
        "            with torch.no_grad():\r\n",
        "              sum_loss=0\r\n",
        "              i=0\r\n",
        "              for x, y in test_loader:\r\n",
        "                    x = x.to(device)\r\n",
        "                    y = y.to(device)\r\n",
        "                    logits = model(x)\r\n",
        "                    sum_loss+= criterion(logits, y).item()\r\n",
        "                    i+=1\r\n",
        "              avrage_loss_array.append((sum_loss/i))\r\n",
        "              train_acc = accuracy(model, train_eval_loader)\r\n",
        "              val_acc = accuracy(model, test_loader)\r\n",
        "              test_acc_array.append(val_acc)\r\n",
        "              # print(test_acc_array)\r\n",
        "              if val_acc > best_acc:\r\n",
        "                  torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\r\n",
        "                  best_acc = val_acc\r\n",
        "              # logger.info(\r\n",
        "              #     \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\r\n",
        "              #     \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\r\n",
        "              #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\r\n",
        "              #         b_nfe_meter.avg, train_acc, val_acc\r\n",
        "              #     )\r\n",
        "              print(\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | Train Acc {:.4f} | Test Acc {:.4f}| test Loss{:.4f}\" .format(\r\n",
        "                      itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\r\n",
        "                      b_nfe_meter.avg, train_acc, val_acc , avrage_loss_array[-1])\r\n",
        "              )\r\n",
        "  return test_acc_array , avrage_loss_array , model\r\n",
        "# test_acc_array , avrage_loss_array , trained_model_normal=fit_model(model_normal)\r\n",
        "# np.save(\"acc_Linear_tanh_normal.npy\" , np.array(test_acc_array))\r\n",
        "# np.save(\"loss_Linear_tanh_normal.npy\" , np.array(avrage_loss_array))\r\n",
        "\r\n",
        "test_acc_array,avrage_loss_array  , trained_model_transpose=fit_model(model_transpose)\r\n",
        "np.save(\"acc_Linear_tanh_transpose.npy\" , np.array(test_acc_array))\r\n",
        "np.save(\"loss_Linear_tanh_transpose.npy\" , np.array(avrage_loss_array))\r\n",
        "\r\n",
        "test_acc_array ,avrage_loss_array, trained_model_baseline=fit_model(model_baseline)\r\n",
        "np.save(\"acc_Linear_tanh_base.npy\" , np.array(test_acc_array))\r\n",
        "np.save(\"loss_Linear_tanh_base.npy\" , np.array(avrage_loss_array))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnVDGjtlOUhv"
      },
      "source": [
        "Linear_tanh_normal=np.load(\"loss_Linear_tanh_normal.npy\" )\r\n",
        "Linear_tanh_transpose=np.load(\"loss_Linear_tanh_transpose.npy\" )\r\n",
        "\r\n",
        "print(len(Linear_tanh_normal))\r\n",
        "print(len(Linear_tanh_transpose))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4qoumcD8lis"
      },
      "source": [
        "Linear_tanh_normal=np.load(\"loss_Linear_tanh_normal.npy\" )\r\n",
        "Linear_tanh_transpose=np.load(\"loss_Linear_tanh_transpose.npy\" )\r\n",
        "Linear_tanh_transpose=np.load(\"loss_Linear_tanh_base.npy\")\r\n",
        "\r\n",
        "first_n_epochs=30\r\n",
        "x=range(len(Linear_tanh_normal))[:first_n_epochs]\r\n",
        "y1=Linear_tanh_normal[:first_n_epochs]\r\n",
        "y2=Linear_tanh_transpose[:first_n_epochs]\r\n",
        "y3=Linear_tanh_transpose[:first_n_epochs]\r\n",
        "plt.plot(x,y1,label=\"Normal weight:    tanh(Wx+b)\")\r\n",
        "plt.plot(x,y2,label=\"Anti-symetric: tanh((W-W^t)x+b)\")\r\n",
        "plt.plot(x,y3,label=\"baseline:  no dinamic \")\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"loss on test set\")\r\n",
        "plt.title(\"experiment with dinamic\")\r\n",
        "# plt.yscale('log')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQSXjHSzoRbX"
      },
      "source": [
        "transpose linear tanh experiment\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9IUa9AC2HZ"
      },
      "source": [
        "# Load a model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bykYCuZFLlgI"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.utils.prune as prune \r\n",
        "args=Args(network='odenet', tol=1e-3,adjoint=True,downsampling_method='conv',nepochs=5,data_aug=True,lr=0.1,batch_size=256,test_batch_size=1000,save='./experimet1',gpu=0 )\r\n",
        "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\r\n",
        "is_odenet = args.network == 'odenet'\r\n",
        "def load_model(checkpoint, model):\r\n",
        "  model.load_state_dict(checkpoint['state_dict'])\r\n",
        "  return model\r\n",
        "\r\n",
        "checkpoint = torch.load(\"odenet_adjoint_50_epoch/model.pth\")\r\n",
        "# print(checkpoint['state_dict'])\r\n",
        "dummie=Ourmodel()\r\n",
        "# print(dummie)\r\n",
        "dummie=load_model(checkpoint, dummie)\r\n",
        "model=dummie.model\r\n",
        "# print(dict(model.feature_layers.named_parameters()))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4D7u4KqW-9n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7sIIwRBDBne"
      },
      "source": [
        "# Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtvV0O3KzYf-"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def glabal_unstructuerd_pruning(pruning_method=prune.RandomUnstructured, index_list_layers=[0,2,4,6,8,11]):\r\n",
        "        prune_params = np.logspace (0,-2,num=100)\r\n",
        "        # prune_params = np.arange(10)/10\r\n",
        "        prune_params=1-prune_params\r\n",
        "        accuracy_array=[None]*len(prune_params)\r\n",
        "        loss_array=[None]*len(prune_params)\r\n",
        "        j=0\r\n",
        "\r\n",
        "        train_loader, test_loader, train_eval_loader = get_mnist_loaders(\r\n",
        "              args.data_aug, args.batch_size, args.test_batch_size\r\n",
        "          )\r\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\r\n",
        "\r\n",
        "        for prune_param in prune_params:\r\n",
        "          torch.manual_seed(50)\r\n",
        "          l2=prune.LnStructured(amount=prune_param,n=2)\r\n",
        "          dummie=Ourmodel()\r\n",
        "          dummie=load_model(checkpoint, dummie)\r\n",
        "          model=dummie.model\r\n",
        "          Parameters_to_prun=((model[0] , 'weight'),\r\n",
        "                              (model[1] , 'weight'),  #norm\r\n",
        "                              (model[3] , 'weight'),  \r\n",
        "                              (model[4] , 'weight'),    #norm\r\n",
        "                              (model[6] , 'weight'),\r\n",
        "                              (model[7].odefunc.norm1 , 'weight'),      #norm\r\n",
        "                              (model[7].odefunc.conv1._layer, 'weight'),\r\n",
        "                              (model[7].odefunc.norm2 , 'weight'),      #norm\r\n",
        "                              (model[7].odefunc.conv2._layer , 'weight'),\r\n",
        "                              (model[7].odefunc.norm3 , 'weight'),    #norm\r\n",
        "                              (model[8] , 'weight'),        #norm\r\n",
        "                              (model[12] , 'weight')\r\n",
        "                              )\r\n",
        "        \r\n",
        "          Parameters_to_prun=tuple(list(list(Parameters_to_prun)[i] for i in index_list_layers))\r\n",
        "          \r\n",
        "\r\n",
        "          prune.global_unstructured(Parameters_to_prun, pruning_method=pruning_method , amount=prune_param,)\r\n",
        "\r\n",
        "          #Is to make the pruning permanent\r\n",
        "          for module , name in Parameters_to_prun:\r\n",
        "            prune.remove(module, name)\r\n",
        "          \r\n",
        "          # is to get the losses and acuracy\r\n",
        "          with torch.no_grad():\r\n",
        "            sum_loss=0\r\n",
        "            sum_acc=0\r\n",
        "            for x, y in test_loader:\r\n",
        "              y_one_hot = one_hot(np.array(y.numpy()), 10)\r\n",
        "              y = y.to(device)\r\n",
        "              x = x.to(device)\r\n",
        "              logits = model(x)\r\n",
        "              sum_loss+= criterion(logits, y).item()\r\n",
        "              target_class = np.argmax(y_one_hot, axis=1)\r\n",
        "              predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\r\n",
        "              sum_acc += np.sum(predicted_class == target_class)\r\n",
        "          accuracy_array[j]=sum_acc/len(test_loader.dataset)\r\n",
        "          loss_array[j]=(sum_loss/len(test_loader.dataset))\r\n",
        "          j+=1\r\n",
        "          print_sparity=False\r\n",
        "          if print_sparity:\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[0].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[0].weight == 0))\r\n",
        "                  / float(model[0].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[1].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[1].weight == 0))\r\n",
        "                  / float(model[1].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[3].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[3].weight == 0))\r\n",
        "                  / float(model[3].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[4].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[4].weight == 0))\r\n",
        "                  / float(model[4].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[6].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[6].weight == 0))\r\n",
        "                  / float(model[6].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[7].odefunc.norm1.weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[7].odefunc.norm1.weight == 0))\r\n",
        "                  / float(model[7].odefunc.norm1.weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[7].odefunc.conv1._layer.weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[7].odefunc.conv1._layer.weight == 0))\r\n",
        "                  / float(model[7].odefunc.conv1._layer.weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[7].odefunc.norm2.weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[7].odefunc.norm2.weight == 0))\r\n",
        "                  / float(model[7].odefunc.norm2.weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[7].odefunc.conv2._layer.weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[7].odefunc.conv2._layer.weight == 0))\r\n",
        "                  / float(model[7].odefunc.conv2._layer.weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[7].odefunc.norm3.weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[7].odefunc.norm3.weight == 0))\r\n",
        "                  / float(model[7].odefunc.norm3.weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[8].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[8].weight == 0))\r\n",
        "                  / float(model[8].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "            print(\r\n",
        "              \"Sparsity in model[12].weight: {:.4f}%\".format(\r\n",
        "                  100. * float(torch.sum(model[12].weight == 0))\r\n",
        "                  / float(model[12].weight.nelement())\r\n",
        "              )\r\n",
        "            )\r\n",
        "        return   accuracy_array ,  loss_array\r\n",
        "\r\n",
        "accuracy_array, loss_array=glabal_unstructuerd_pruning(pruning_method=prune.RandomUnstructured, index_list_layers=[0,1,2,3,4,5,6,7,8,9,10,11])\r\n",
        "np.save(\"acc_RandomUnstructered_all weights and norms.npy\" , np.array(accuracy_array))\r\n",
        "np.save(\"loss_RandomUnstructered_all weights end norms.npy\" , np.array(loss_array))\r\n",
        "\r\n",
        "accuracy_array, loss_array=glabal_unstructuerd_pruning(pruning_method=prune.RandomUnstructured, index_list_layers=[0,2,4,6,8,11])\r\n",
        "np.save(\"acc_RandomUnstructered_all weights.npy\" , np.array(accuracy_array))\r\n",
        "np.save(\"loss_RandomUnstructered_all weights.npy\" , np.array(loss_array))\r\n",
        "\r\n",
        "accuracy_array, loss_array=glabal_unstructuerd_pruning(pruning_method=prune.RandomStructured, index_list_layers=[0,2,4,6,8,11])\r\n",
        "np.save(\"acc_RandomStructured_all weights.npy\" , np.array(accuracy_array))\r\n",
        "np.save(\"loss_RandomStructured_all weights.npy\" , np.array(loss_array))\r\n",
        "\r\n",
        "accuracy_array, loss_array=glabal_unstructuerd_pruning(pruning_method=prune.RandomUnstructured, index_list_layers=[6,8])\r\n",
        "np.save(\"acc_RandomUnstructered_only_dynamic_weights.npy\" , np.array(accuracy_array))\r\n",
        "np.save(\"loss_RandomUnstructered_only_dynamic_weights.npy\" , np.array(loss_array))\r\n",
        "\r\n",
        "accuracy_array, loss_array=glabal_unstructuerd_pruning(pruning_method=prune.L1Unstructured, index_list_layers=[0,1,2,3,4,5,6,7,8,9,10,11])\r\n",
        "np.save(\"acc_L1Unstructered_all weights and norms.npy\" , np.array(accuracy_array))\r\n",
        "np.save(\"loss_L1Unstructered_all weights end norms.npy\" , np.array(loss_array))\r\n",
        "\r\n",
        "accuracy_array, loss_array =glabal_unstructuerd_pruning(pruning_method=prune.L1Unstructured, index_list_layers=[0,2,4,6,8,11])\r\n",
        "np.save(\"acc_L1Unstructered_all weights.npy\" , np.array(accuracy_array))\r\n",
        "np.save(\"loss_L1Unstructered_all weights.npy\" , np.array(loss_array))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(accuracy_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_xSgHAbSDE-"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        " # print(prune.is_pruned(model))\r\n",
        "  # print(model)\r\n",
        "  # print(model._forward_pre_hooks)\r\n",
        "  # print(model.named_parameters())\r\n",
        "  # print(model.named_buffers())\r\n",
        "  # for hook in module._forward_pre_hooks.values():\r\n",
        "  #   if hook._tensor_name == \"weight\":  # select out the correct hook hook is a costumfromMask opject\r\n",
        "  #     print(hook)\r\n",
        "  #     break\\\r\n",
        "\r\n",
        "  # for name , module in model.named_modules():\r\n",
        "  #   if name!='':\r\n",
        "  #     print(name)\r\n",
        "  #     print(module.state_dict().keys())\r\n",
        "  #     print(len(module.state_dict().keys()))\r\n",
        "\r\n",
        "  \r\n",
        "  # print(list(model.named_modules()))\r\n",
        "  # for name in model.state_dict().keys():\r\n",
        "  #   if 'weight' in name:\r\n",
        "  #     print(name) \r\n",
        "  # print(model.state_dict())\r\n",
        "  # print(list(model.named_parameters()))\r\n",
        "  # print(list(model.named_buffers()))\r\n",
        "  # print(count_parameters(model))\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTUgbbD-JN-_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0fr4YmoL8pK"
      },
      "source": [
        "# random pruning\r\n",
        "acc_RandomUnstructered_all_weights_and_norms=np.load(\"acc_RandomUnstructered_all weights and norms.npy\" )\r\n",
        "acc_RandomUnstructered_all_weights=np.load(\"acc_RandomUnstructered_all weights.npy\")\r\n",
        "acc_RandomUnstructered_only_dynamic_weights=np.load(\"acc_RandomUnstructered_only_dynamic_weights.npy\" )\r\n",
        "\r\n",
        "#L1pruning \r\n",
        "acc_L1Unstructered_all_weights_and_norms=np.load(\"acc_L1Unstructered_all weights and norms.npy\" )\r\n",
        "acc_L1Unstructered_all_weights=np.load(\"acc_L1Unstructered_all weights.npy\" )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qq3DXSnwLyb"
      },
      "source": [
        "acc_tuple=( \r\n",
        "            (\"Random Weights and Norms\",acc_RandomUnstructered_all_weights_and_norms),\r\n",
        "            (\"Random Weights\" , acc_RandomUnstructered_all_weights) , \r\n",
        "            (\"Random only Dynamic weights \" , acc_RandomUnstructered_only_dynamic_weights) , \r\n",
        "            (\"L1 Weights and Norms\" , acc_L1Unstructered_all_weights_and_norms), \r\n",
        "            (\"L1 Weights\" , acc_L1Unstructered_all_weights)\r\n",
        "          )\r\n",
        "\r\n",
        "prune_params = np.logspace (0,-2,num=100)\r\n",
        "prune_params=1-prune_params\r\n",
        "x=prune_params\r\n",
        "for label , array in acc_tuple:\r\n",
        "    plt.plot(x,array,label=label)\r\n",
        "plt.xlabel(\"proporsion that is left\")\r\n",
        "plt.ylabel(\"accuracy on test set\")\r\n",
        "plt.title(\"Title\")\r\n",
        "plt.xscale('logit')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8C0gcgrz8p2"
      },
      "source": [
        "# Print model's state_dict\r\n",
        "print(\"Model's state_dict:\")\r\n",
        "for param_tensor in model.state_dict():\r\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPix4J70GaAy"
      },
      "source": [
        "### Pruning per layer \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4y64wHGYu4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "prune_params = np.logspace (0,-2,num=50)\r\n",
        "prune_params=1-prune_params\r\n",
        "accuracy_array1=[]\r\n",
        "accuracy_array2=[]\r\n",
        "saved=False\r\n",
        "for prune_param in prune_params:\r\n",
        "  # print('parameter')\r\n",
        "  # print(prune_param)\r\n",
        "  torch.manual_seed(5050)\r\n",
        "  dummie1=Ourmodel()\r\n",
        "  dummie2=Ourmodel()\r\n",
        "  dummie1=load_model(checkpoint, dummie1)\r\n",
        "  dummie2=load_model(checkpoint, dummie2)\r\n",
        "  model1=dummie1.model\r\n",
        "  model2=dummie2.model\r\n",
        "  # val_acc = accuracy(model, test_loader)\r\n",
        "  # print(val_acc)\r\n",
        "  for param_tensor in model1.state_dict():\r\n",
        "        if \"weight\" in param_tensor:\r\n",
        "          # if prune_param==0.:\r\n",
        "            # print(param_tensor)\r\n",
        "          if param_tensor[1]=='.':\r\n",
        "            a=int(param_tensor[0])\r\n",
        "            i=0\r\n",
        "          else:\r\n",
        "            a=int(param_tensor[:2])\r\n",
        "            i=1\r\n",
        "          # print(model[a])\r\n",
        "          if a==100:\r\n",
        "            prune.random_unstructured(model1[a], name=param_tensor[i+2:], amount=prune_param)\r\n",
        "            val_acc1 = accuracy(model1, test_loader)\r\n",
        "            prune.l1_unstructured(model2[a], name=param_tensor[i+2:], amount=prune_param)\r\n",
        "            val_acc2 = accuracy(model2, test_loader)\r\n",
        "          # if a==7:\r\n",
        "              # prune.random_unstructured(model[a].odefunc.norm1, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.random_unstructured(model[a].odefunc.conv1._layer, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.random_unstructured(model[a].odefunc.norm2, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.random_unstructured(model[a].odefunc.conv2._layer, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.random_unstructured(model[a].odefunc.norm3, name=param_tensor[-6:], amount=prune_param)\r\n",
        "\r\n",
        "              # prune.l1_unstructured(model[a].odefunc.norm1, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.l1_unstructured(model[a].odefunc.conv1._layer, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.l1_unstructured(model[a].odefunc.norm2, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.l1_unstructured(model[a].odefunc.conv2._layer, name=param_tensor[-6:], amount=prune_param)\r\n",
        "              # prune.l1_unstructured(model[a].odefunc.norm3, name=param_tensor[-6:], amount=prune_param)\r\n",
        " \r\n",
        "  accuracy_array1.append(val_acc1)\r\n",
        "  accuracy_array2.append(val_acc2)\r\n",
        "print(prune_params)\r\n",
        "print(accuracy_array1)\r\n",
        "print(accuracy_array2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA5FFtUQtnpR"
      },
      "source": [
        "ODE code for MNIST data set in blocks \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyH_LiN1WSa7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-gfS816twOX"
      },
      "source": [
        "# makedirs(args.save)\r\n",
        "# logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\r\n",
        "# logger.info(args)\r\n",
        "args=Args(network='odenet', tol=1e-3,adjoint=False,downsampling_method='conv',nepochs=5,data_aug=True,lr=0.1,batch_size=254,test_batch_size=1000,save='./experimet1',gpu=0 )\r\n",
        "\r\n",
        "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "is_odenet = args.network == 'odenet'\r\n",
        "\r\n",
        "model=Ourmodel()\r\n",
        "\r\n",
        "\r\n",
        "# if args.downsampling_method == 'conv':\r\n",
        "#   downsampling_layers = [\r\n",
        "#     nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1 , padding=0),     #so we get a 64*26*26 tensor\r\n",
        "#     norm(64),\r\n",
        "#     nn.ReLU(inplace=True),\r\n",
        "#     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),     # gives back a 64*12*12 tensor\r\n",
        "#     norm(64),\r\n",
        "#     nn.ReLU(inplace=True),\r\n",
        "#     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1),     #gives a 62*5*5 tensor\r\n",
        "#     ]\r\n",
        "# elif args.downsampling_method == 'res':\r\n",
        "#     downsampling_layers = [\r\n",
        "#         nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1),             \r\n",
        "#         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\r\n",
        "#         ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\r\n",
        "#         ]\r\n",
        "\r\n",
        "# feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\r\n",
        "# fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\r\n",
        "\r\n",
        "# model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\r\n",
        "\r\n",
        "# logger.info(model)\r\n",
        "# logger.info('Number of parameters: {}'.format(count_parameters(model)))\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss().to(device)\r\n",
        "\r\n",
        "train_loader, test_loader, train_eval_loader = get_mnist_loaders(\r\n",
        "    args.data_aug, args.batch_size, args.test_batch_size\r\n",
        ")\r\n",
        "\r\n",
        "data_gen = inf_generator(train_loader)\r\n",
        "batches_per_epoch = len(train_loader)\r\n",
        "\r\n",
        "lr_fn = learning_rate_with_decay(\r\n",
        "    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\r\n",
        "    decay_rates=[1, 0.1, 0.01, 0.001]\r\n",
        ")\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\r\n",
        "\r\n",
        "best_acc = 0\r\n",
        "batch_time_meter = RunningAverageMeter()\r\n",
        "f_nfe_meter = RunningAverageMeter()\r\n",
        "b_nfe_meter = RunningAverageMeter()\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "train=False\r\n",
        "if train:\r\n",
        "  for itr in range(args.nepochs * batches_per_epoch):\r\n",
        "\r\n",
        "      for param_group in optimizer.param_groups:\r\n",
        "          param_group['lr'] = lr_fn(itr)\r\n",
        "\r\n",
        "      optimizer.zero_grad()\r\n",
        "      x, y = data_gen.__next__()\r\n",
        "      x = x.to(device)\r\n",
        "      y = y.to(device)\r\n",
        "      logits = model(x)\r\n",
        "      loss = criterion(logits, y)\r\n",
        "\r\n",
        "      if is_odenet:\r\n",
        "          nfe_forward = model.feature_layers[0].nfe\r\n",
        "          model.feature_layers[0].nfe = 0\r\n",
        "\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "      if is_odenet:\r\n",
        "          nfe_backward = model.feature_layers[0].nfe\r\n",
        "          model.feature_layers[0].nfe = 0\r\n",
        "\r\n",
        "      batch_time_meter.update(time.time() - end)\r\n",
        "      if is_odenet:\r\n",
        "          f_nfe_meter.update(nfe_forward)\r\n",
        "          b_nfe_meter.update(nfe_backward)\r\n",
        "      end = time.time()\r\n",
        "\r\n",
        "      if itr % batches_per_epoch == 0:\r\n",
        "          with torch.no_grad():\r\n",
        "              train_acc = accuracy(model, train_eval_loader)\r\n",
        "              val_acc = accuracy(model, test_loader)\r\n",
        "              if val_acc > best_acc:\r\n",
        "                  torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\r\n",
        "                  best_acc = val_acc\r\n",
        "              # logger.info(\r\n",
        "              #     \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\r\n",
        "              #     \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\r\n",
        "              #         itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\r\n",
        "              #         b_nfe_meter.avg, train_acc, val_acc\r\n",
        "              #     )\r\n",
        "              print(\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | Train Acc {:.4f} | Test Acc {:.4f}\".format(\r\n",
        "                      itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\r\n",
        "                      b_nfe_meter.avg, train_acc, val_acc)\r\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZWLm-raPEaI"
      },
      "source": [
        "This is to see some predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGsPcK7LVQG"
      },
      "source": [
        "from torchvision.datasets import MNIST\r\n",
        "dataset = MNIST(root='data/', download=True)\r\n",
        "dataset = MNIST(root='data/', \r\n",
        "                train=True,\r\n",
        "                transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63KTuNsUeax"
      },
      "source": [
        "img_tensor, label = dataset[190]\r\n",
        "print(img_tensor.shape)\r\n",
        "x=img_tensor\r\n",
        "x=x.unsqueeze(0)\r\n",
        "y=torch.tensor([label])\r\n",
        "\r\n",
        "train_loader, test_loader, train_eval_loader = get_mnist_loaders(\r\n",
        "    args.data_aug, args.batch_size, args.test_batch_size\r\n",
        ")\r\n",
        "data_gen = inf_generator(train_loader)\r\n",
        "\r\n",
        "x, y = data_gen.__next__()\r\n",
        "x = x.to(device)\r\n",
        "y=y.to(device)\r\n",
        "# print(x.shape)\r\n",
        "# print(y.shape)\r\n",
        "model=model.to(device)\r\n",
        "logits = model(x)\r\n",
        "predicted_class = np.argmax(logits.cpu().detach().numpy(), axis=1)\r\n",
        "loss = criterion(logits, y)\r\n",
        "loss.backward()\r\n",
        "# print(model.feature_layers[0])\r\n",
        "# for param_tensor in model.state_dict():\r\n",
        "#     if \"weight\" in param_tensor:\r\n",
        "#       # model.state_dict()[param_tensor]=model.state_dict()[param_tensor].prune(method='random', amount=8)\r\n",
        "#       print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\r\n",
        "# # print(y)\r\n",
        "# print(predicted_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OycZovguSbv5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umj3nFIy0z_U"
      },
      "source": [
        "# Print model's state_dict\r\n",
        "print(\"Model's state_dict:\")\r\n",
        "for param_tensor in model.state_dict():\r\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\r\n",
        "\r\n",
        "# Print optimizer's state_dict\r\n",
        "# print(\"Optimizer's state_dict:\")\r\n",
        "# for var_name in optimizer.state_dict():\r\n",
        "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz6oAxk-tz2v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdOBh0N-sOxr"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "input = torch.randn(20, 6)\r\n",
        "print(input.size())\r\n",
        "m = nn.GroupNorm(1, 6)\r\n",
        "m(input).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQ1ffdDsVNI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}